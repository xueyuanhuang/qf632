{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0710f653",
   "metadata": {},
   "source": [
    "Regex (regular expressions) played a crucial role in the early days of NLP (Natural Language Processing) before the advent of modern models like transformers:\n",
    "\n",
    "1. Pattern Matching:\n",
    "Regex provided a powerful and efficient way to identify and extract patterns in text. This was especially useful for tasks such as:\n",
    "Information Extraction: Identifying names, dates, email addresses, and other specific data points.\n",
    "Text Cleaning: Removing unwanted characters, whitespace, and formatting artifacts from text data.\n",
    "\n",
    "2. Simplicity and Speed:\n",
    "Regex operations are generally fast and efficient. They can be executed quickly and with minimal computational resources, making them ideal for early NLP systems with limited processing power and memory.\n",
    "\n",
    "3. Linguistic Rule Implementation:\n",
    "Before sophisticated machine learning models, much of NLP relied on hand-crafted linguistic rules. Regex allowed developers to encode these rules in a concise and maintainable way, facilitating tasks like:\n",
    "Tokenization: Splitting text into words, sentences, or other meaningful units.\n",
    "Normalization: Converting text to a standard format, such as lowercasing or stemming words.\n",
    "\n",
    "4. Text Classification and Preprocessing:\n",
    "Regex was essential for preprocessing text data for various NLP tasks. By using regex, developers could:\n",
    "Filter Text: Remove noise or irrelevant parts of text data.\n",
    "Feature Extraction: Generate features based on text patterns for use in simpler statistical models like Naive Bayes or Logistic Regression.\n",
    "\n",
    "5. Lack of Data and Resources:\n",
    "In the early days, large annotated datasets were scarce, and computing resources were limited. Regex allowed NLP practitioners to perform meaningful text manipulation and analysis without the need for extensive training data or powerful hardware.\n",
    "\n",
    "6. Flexibility and Generality:\n",
    "Regex is a general tool that can be applied to any language or text format. It provided a flexible approach for handling diverse text data, which was crucial before the development of specialized NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2a4f2",
   "metadata": {},
   "source": [
    "Explanation of Concepts\n",
    "Matching Literal Strings:\n",
    "Finds exact matches of the string \"fox\".\n",
    "\n",
    "Using Metacharacters:\n",
    "\\b\\w{3}\\b matches words of exactly 3 letters.\n",
    "\n",
    "Character Classes:\n",
    "[a-zA-Z]+ matches words containing only letters.\n",
    "\n",
    "Predefined Character Classes:\n",
    "\\d{3}-\\d{3}-\\d{3} matches a pattern similar to a phone number.\n",
    "\n",
    "Anchors:\n",
    "^The matches \"The\" at the start of the string.\n",
    "\n",
    "Groups and Capturing:\n",
    "(\\w+)@(\\w+\\.\\w+) captures the username and domain of an email separately.\n",
    "\n",
    "Non-capturing Groups:\n",
    "(?:\\+\\d{1,3}-)?\\d{3}-\\d{3}-\\d{3} matches phone numbers with an optional country code.\n",
    "\n",
    "Lookahead and Lookbehind:\n",
    "\\b\\w+(?=\\sfox) matches a word that appears before the word \"fox\".\n",
    "(?<=\\bquick\\s)\\w+ matches a word that appears after the word \"quick\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20cd46f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Matching Literal Strings:\n",
      " ['fox']\n",
      "\n",
      "2. Using Metacharacters:\n",
      " ['The', 'fox', 'the', 'dog', 'com', '234', '567', '890']\n",
      "\n",
      "3. Character Classes:\n",
      " ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'Email', 'example', 'example', 'com', 'Phone']\n",
      "\n",
      "4. Predefined Character Classes:\n",
      " ['234-567-890']\n",
      "\n",
      "5. Anchors:\n",
      " ['The']\n",
      "\n",
      "6. Groups and Capturing:\n",
      " [('example', 'example.com')]\n",
      "\n",
      "7. Non-capturing Groups:\n",
      " ['+1-234-567-890']\n",
      "\n",
      "8. Lookahead:\n",
      " ['brown']\n",
      "\n",
      "8. Lookbehind:\n",
      " ['brown']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog. Email: example@example.com. Phone: +1-234-567-890.\"\n",
    "\n",
    "# 1. Matching Literal Strings\n",
    "pattern_literal = re.compile(r\"fox\")\n",
    "matches_literal = pattern_literal.findall(text)\n",
    "print(\"1. Matching Literal Strings:\\n\", matches_literal)\n",
    "\n",
    "# 2. Using Metacharacters (e.g., ., ^, $, *, +, ?, {}, [], |, \\)\n",
    "pattern_meta = re.compile(r\"\\b\\w{3}\\b\")  # Words of exactly 3 letters\n",
    "matches_meta = pattern_meta.findall(text)\n",
    "print(\"\\n2. Using Metacharacters:\\n\", matches_meta)\n",
    "\n",
    "# 3. Character Classes\n",
    "pattern_class = re.compile(r\"[a-zA-Z]+\")  # Words containing only letters\n",
    "matches_class = pattern_class.findall(text)\n",
    "print(\"\\n3. Character Classes:\\n\", matches_class)\n",
    "\n",
    "# 4. Predefined Character Classes (e.g., \\d, \\D, \\s, \\S, \\w, \\W)\n",
    "pattern_predefined = re.compile(r\"\\d{3}-\\d{3}-\\d{3}\")  # Phone number pattern\n",
    "matches_predefined = pattern_predefined.findall(text)\n",
    "print(\"\\n4. Predefined Character Classes:\\n\", matches_predefined)\n",
    "\n",
    "# 5. Anchors (e.g., ^, $)\n",
    "pattern_anchor = re.compile(r\"^The\")  # Matches 'The' at the start of the string\n",
    "matches_anchor = pattern_anchor.findall(text)\n",
    "print(\"\\n5. Anchors:\\n\", matches_anchor)\n",
    "\n",
    "# 6. Groups and Capturing\n",
    "pattern_group = re.compile(r\"(\\w+)@(\\w+\\.\\w+)\")  # Captures username and domain separately\n",
    "matches_group = pattern_group.findall(text)\n",
    "print(\"\\n6. Groups and Capturing:\\n\", matches_group)\n",
    "\n",
    "# 7. Non-capturing Groups\n",
    "pattern_non_capturing = re.compile(r\"(?:\\+\\d{1,3}-)?\\d{3}-\\d{3}-\\d{3}\")  # Matches phone number with optional country code\n",
    "matches_non_capturing = pattern_non_capturing.findall(text)\n",
    "print(\"\\n7. Non-capturing Groups:\\n\", matches_non_capturing)\n",
    "\n",
    "# 8. Lookahead and Lookbehind\n",
    "pattern_lookahead = re.compile(r\"\\b\\w+(?=\\sfox)\")  # Matches word before 'fox'\n",
    "matches_lookahead = pattern_lookahead.findall(text)\n",
    "print(\"\\n8. Lookahead:\\n\", matches_lookahead)\n",
    "\n",
    "pattern_lookbehind = re.compile(r\"(?<=\\bquick\\s)\\w+\")  # Matches word after 'quick'\n",
    "matches_lookbehind = pattern_lookbehind.findall(text)\n",
    "print(\"\\n8. Lookbehind:\\n\", matches_lookbehind)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114fae84",
   "metadata": {},
   "source": [
    "Tokenization:\n",
    "Sentence Tokenization: Splitting the text into sentences using sent_tokenize.\n",
    "Word Tokenization: Splitting the text into words using word_tokenize.\n",
    "\n",
    "Stemming:\n",
    "Using PorterStemmer to reduce words to their root form. For example, \"running\" becomes \"run\".\n",
    "\n",
    "Lemmatization:\n",
    "Using WordNetLemmatizer to reduce words to their base form (lemma). For example, \"running\" becomes \"run\", but unlike stemming, lemmatization considers the context and converts words to meaningful base forms.\n",
    "\n",
    "Stopwords Removal:\n",
    "Removing common words (stopwords) that may not contribute significant meaning to the text. The list of stopwords is obtained from nltk.corpus.stopwords.\n",
    "\n",
    "Keyword Dictionaries:\n",
    "Using a predefined dictionary (keywords) to map certain words to specific categories or keywords. For example, \"river\" is mapped to \"water_body\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5789110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Sentence Tokenization:\n",
      " ['The Kennebec River now runs mostly clean, thanks to laws that reduced pollution.', 'Yet four hydroelectric dams, two built in the early 20th century and the other two in the 1980s, remain on the lower reaches of the 150-mile-long river and continue to prevent endangered salmon from reaching their single most important spawning tributary, the Sandy River.']\n",
      "\n",
      "1. Word Tokenization:\n",
      " ['The', 'Kennebec', 'River', 'now', 'runs', 'mostly', 'clean', ',', 'thanks', 'to', 'laws', 'that', 'reduced', 'pollution', '.', 'Yet', 'four', 'hydroelectric', 'dams', ',', 'two', 'built', 'in', 'the', 'early', '20th', 'century', 'and', 'the', 'other', 'two', 'in', 'the', '1980s', ',', 'remain', 'on', 'the', 'lower', 'reaches', 'of', 'the', '150-mile-long', 'river', 'and', 'continue', 'to', 'prevent', 'endangered', 'salmon', 'from', 'reaching', 'their', 'single', 'most', 'important', 'spawning', 'tributary', ',', 'the', 'Sandy', 'River', '.']\n",
      "\n",
      "2. Stemming:\n",
      " ['the', 'kennebec', 'river', 'now', 'run', 'mostli', 'clean', ',', 'thank', 'to', 'law', 'that', 'reduc', 'pollut', '.', 'yet', 'four', 'hydroelectr', 'dam', ',', 'two', 'built', 'in', 'the', 'earli', '20th', 'centuri', 'and', 'the', 'other', 'two', 'in', 'the', '1980', ',', 'remain', 'on', 'the', 'lower', 'reach', 'of', 'the', '150-mile-long', 'river', 'and', 'continu', 'to', 'prevent', 'endang', 'salmon', 'from', 'reach', 'their', 'singl', 'most', 'import', 'spawn', 'tributari', ',', 'the', 'sandi', 'river', '.']\n",
      "\n",
      "3. Lemmatization:\n",
      " ['The', 'Kennebec', 'River', 'now', 'run', 'mostly', 'clean', ',', 'thanks', 'to', 'law', 'that', 'reduced', 'pollution', '.', 'Yet', 'four', 'hydroelectric', 'dam', ',', 'two', 'built', 'in', 'the', 'early', '20th', 'century', 'and', 'the', 'other', 'two', 'in', 'the', '1980s', ',', 'remain', 'on', 'the', 'lower', 'reach', 'of', 'the', '150-mile-long', 'river', 'and', 'continue', 'to', 'prevent', 'endangered', 'salmon', 'from', 'reaching', 'their', 'single', 'most', 'important', 'spawning', 'tributary', ',', 'the', 'Sandy', 'River', '.']\n",
      "\n",
      "4. Stopwords Removal:\n",
      " ['Kennebec', 'River', 'runs', 'mostly', 'clean', ',', 'thanks', 'laws', 'reduced', 'pollution', '.', 'Yet', 'four', 'hydroelectric', 'dams', ',', 'two', 'built', 'early', '20th', 'century', 'two', '1980s', ',', 'remain', 'lower', 'reaches', '150-mile-long', 'river', 'continue', 'prevent', 'endangered', 'salmon', 'reaching', 'single', 'important', 'spawning', 'tributary', ',', 'Sandy', 'River', '.']\n",
      "\n",
      "5. Keyword Dictionaries:\n",
      " {'River': 'water_body', 'pollution': 'environment_issue', 'river': 'water_body', 'salmon': 'fish'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Example text\n",
    "text = (\"The Kennebec River now runs mostly clean, thanks to laws that reduced pollution. \"\n",
    "        \"Yet four hydroelectric dams, two built in the early 20th century and the other two in the 1980s, \"\n",
    "        \"remain on the lower reaches of the 150-mile-long river and continue to prevent endangered salmon \"\n",
    "        \"from reaching their single most important spawning tributary, the Sandy River.\")\n",
    "\n",
    "# 1. Tokenization\n",
    "# Sentence Tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"1. Sentence Tokenization:\\n\", sentences)\n",
    "\n",
    "# Word Tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"\\n1. Word Tokenization:\\n\", words)\n",
    "\n",
    "# 2. Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "print(\"\\n2. Stemming:\\n\", stemmed_words)\n",
    "\n",
    "# 3. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"\\n3. Lemmatization:\\n\", lemmatized_words)\n",
    "\n",
    "# 4. Stopwords Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "print(\"\\n4. Stopwords Removal:\\n\", filtered_words)\n",
    "\n",
    "# 5. Keyword Dictionaries\n",
    "keywords = {\"river\": \"water_body\", \"pollution\": \"environment_issue\", \"salmon\": \"fish\"}\n",
    "keyword_dict = {word: keywords[word.lower()] for word in words if word.lower() in keywords}\n",
    "print(\"\\n5. Keyword Dictionaries:\\n\", keyword_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91577c45",
   "metadata": {},
   "source": [
    "One-Hot Encoding:\n",
    "CountVectorizer(binary=True) creates a binary vector for each word.\n",
    "The vocabulary is displayed to show the mapping of words to their indices.\n",
    "\n",
    "Bag-of-Words:\n",
    "CountVectorizer() creates a frequency vector for each word.\n",
    "The vocabulary is displayed to show the mapping of words to their indices.\n",
    "\n",
    "TF-IDF:\n",
    "TfidfVectorizer() creates a TF-IDF vector for each word.\n",
    "The vocabulary is displayed to show the mapping of words to their indices.\n",
    "compute_tf function calculates term frequency (TF) manually.\n",
    "compute_idf function calculates inverse document frequency (IDF) manually.\n",
    "\n",
    "Word2Vec CBOW:\n",
    "Word2Vec model with sg=0 (CBOW) creates word vectors.\n",
    "An example word vector for 'river' is displayed.\n",
    "\n",
    "Word2Vec Skip-gram:\n",
    "Word2Vec model with sg=1 (Skip-gram) creates word vectors.\n",
    "An example word vector for 'river' is displayed.\n",
    "\n",
    "Average Word Embeddings:\n",
    "Function average_word_vectors computes the average vector for a sentence.\n",
    "The average word embedding for the example text is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac65be3-e4cc-4b98-b2ba-ef4b52151471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding:\n",
      " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1]]\n",
      "\n",
      "One-Hot Encoding Vocabulary:\n",
      " {'the': 41, 'kennebec': 16, 'river': 33, 'now': 23, 'runs': 34, 'mostly': 22, 'clean': 6, 'thanks': 39, 'to': 43, 'laws': 17, 'that': 40, 'reduced': 31, 'pollution': 27, 'yet': 46, 'four': 11, 'hydroelectric': 13, 'dams': 8, 'two': 45, 'built': 4, 'in': 15, 'early': 9, '20th': 2, 'century': 5, 'and': 3, 'other': 26, '1980s': 1, 'remain': 32, 'on': 25, 'lower': 19, 'reaches': 29, 'of': 24, '150': 0, 'mile': 20, 'long': 18, 'continue': 7, 'prevent': 28, 'endangered': 10, 'salmon': 35, 'from': 12, 'reaching': 30, 'their': 42, 'single': 37, 'most': 21, 'important': 14, 'spawning': 38, 'tributary': 44, 'sandy': 36}\n",
      "\n",
      "Bag-of-Words:\n",
      " [[1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1\n",
      "  1 1 1 1 1 7 1 2 1 2 1]]\n",
      "\n",
      "Bag-of-Words Vocabulary:\n",
      " {'the': 41, 'kennebec': 16, 'river': 33, 'now': 23, 'runs': 34, 'mostly': 22, 'clean': 6, 'thanks': 39, 'to': 43, 'laws': 17, 'that': 40, 'reduced': 31, 'pollution': 27, 'yet': 46, 'four': 11, 'hydroelectric': 13, 'dams': 8, 'two': 45, 'built': 4, 'in': 15, 'early': 9, '20th': 2, 'century': 5, 'and': 3, 'other': 26, '1980s': 1, 'remain': 32, 'on': 25, 'lower': 19, 'reaches': 29, 'of': 24, '150': 0, 'mile': 20, 'long': 18, 'continue': 7, 'prevent': 28, 'endangered': 10, 'salmon': 35, 'from': 12, 'reaching': 30, 'their': 42, 'single': 37, 'most': 21, 'important': 14, 'spawning': 38, 'tributary': 44, 'sandy': 36}\n",
      "\n",
      "TF-IDF:\n",
      " [[0.09325048 0.09325048 0.09325048 0.18650096 0.09325048 0.09325048\n",
      "  0.09325048 0.09325048 0.09325048 0.09325048 0.09325048 0.09325048\n",
      "  0.09325048 0.09325048 0.09325048 0.18650096 0.09325048 0.09325048\n",
      "  0.09325048 0.09325048 0.09325048 0.09325048 0.09325048 0.09325048\n",
      "  0.09325048 0.09325048 0.09325048 0.09325048 0.09325048 0.09325048\n",
      "  0.09325048 0.09325048 0.09325048 0.27975144 0.09325048 0.09325048\n",
      "  0.09325048 0.09325048 0.09325048 0.09325048 0.09325048 0.65275337\n",
      "  0.09325048 0.18650096 0.09325048 0.18650096 0.09325048]]\n",
      "\n",
      "TF-IDF Vocabulary:\n",
      " {'the': 41, 'kennebec': 16, 'river': 33, 'now': 23, 'runs': 34, 'mostly': 22, 'clean': 6, 'thanks': 39, 'to': 43, 'laws': 17, 'that': 40, 'reduced': 31, 'pollution': 27, 'yet': 46, 'four': 11, 'hydroelectric': 13, 'dams': 8, 'two': 45, 'built': 4, 'in': 15, 'early': 9, '20th': 2, 'century': 5, 'and': 3, 'other': 26, '1980s': 1, 'remain': 32, 'on': 25, 'lower': 19, 'reaches': 29, 'of': 24, '150': 0, 'mile': 20, 'long': 18, 'continue': 7, 'prevent': 28, 'endangered': 10, 'salmon': 35, 'from': 12, 'reaching': 30, 'their': 42, 'single': 37, 'most': 21, 'important': 14, 'spawning': 38, 'tributary': 44, 'sandy': 36}\n",
      "\n",
      "TF:\n",
      " [{'the': 0.1111111111111111, 'kennebec': 0.015873015873015872, 'river': 0.047619047619047616, 'now': 0.015873015873015872, 'runs': 0.015873015873015872, 'mostly': 0.015873015873015872, 'clean': 0.015873015873015872, ',': 0.06349206349206349, 'thanks': 0.015873015873015872, 'to': 0.031746031746031744, 'laws': 0.015873015873015872, 'that': 0.015873015873015872, 'reduced': 0.015873015873015872, 'pollution': 0.015873015873015872, '.': 0.031746031746031744, 'yet': 0.015873015873015872, 'four': 0.015873015873015872, 'hydroelectric': 0.015873015873015872, 'dams': 0.015873015873015872, 'two': 0.031746031746031744, 'built': 0.015873015873015872, 'in': 0.031746031746031744, 'early': 0.015873015873015872, '20th': 0.015873015873015872, 'century': 0.015873015873015872, 'and': 0.031746031746031744, 'other': 0.015873015873015872, '1980s': 0.015873015873015872, 'remain': 0.015873015873015872, 'on': 0.015873015873015872, 'lower': 0.015873015873015872, 'reaches': 0.015873015873015872, 'of': 0.015873015873015872, '150-mile-long': 0.015873015873015872, 'continue': 0.015873015873015872, 'prevent': 0.015873015873015872, 'endangered': 0.015873015873015872, 'salmon': 0.015873015873015872, 'from': 0.015873015873015872, 'reaching': 0.015873015873015872, 'their': 0.015873015873015872, 'single': 0.015873015873015872, 'most': 0.015873015873015872, 'important': 0.015873015873015872, 'spawning': 0.015873015873015872, 'tributary': 0.015873015873015872, 'sandy': 0.015873015873015872}]\n",
      "\n",
      "IDF:\n",
      " {'spawning': -0.6931471805599453, 'runs': -0.6931471805599453, ',': -0.6931471805599453, 'four': -0.6931471805599453, '1980s': -0.6931471805599453, 'sandy': -0.6931471805599453, 'remain': -0.6931471805599453, 'built': -0.6931471805599453, 'on': -0.6931471805599453, 'endangered': -0.6931471805599453, 'their': -0.6931471805599453, 'important': -0.6931471805599453, 'continue': -0.6931471805599453, 'tributary': -0.6931471805599453, 'of': -0.6931471805599453, 'hydroelectric': -0.6931471805599453, 'reaches': -0.6931471805599453, 'thanks': -0.6931471805599453, 'yet': -0.6931471805599453, 'two': -0.6931471805599453, 'reaching': -0.6931471805599453, 'and': -0.6931471805599453, 'lower': -0.6931471805599453, 'century': -0.6931471805599453, 'other': -0.6931471805599453, 'single': -0.6931471805599453, 'dams': -0.6931471805599453, 'in': -0.6931471805599453, 'now': -0.6931471805599453, 'laws': -0.6931471805599453, '.': -0.6931471805599453, 'most': -0.6931471805599453, 'pollution': -0.6931471805599453, 'the': -0.6931471805599453, 'early': -0.6931471805599453, 'mostly': -0.6931471805599453, '150-mile-long': -0.6931471805599453, 'to': -0.6931471805599453, 'reduced': -0.6931471805599453, 'clean': -0.6931471805599453, 'salmon': -0.6931471805599453, '20th': -0.6931471805599453, 'that': -0.6931471805599453, 'prevent': -0.6931471805599453, 'river': -0.6931471805599453, 'kennebec': -0.6931471805599453, 'from': -0.6931471805599453}\n",
      "\n",
      "Word2Vec CBOW example for 'river':\n",
      " [ 9.6799791e-05  3.0772467e-03 -6.8096868e-03 -1.3777065e-03\n",
      "  7.6681329e-03  7.3419139e-03 -3.6734371e-03  2.6478192e-03\n",
      " -8.3197244e-03  6.2001119e-03 -4.6338448e-03 -3.1682157e-03\n",
      "  9.3134725e-03  8.7393710e-04  7.4930806e-03 -6.0711070e-03\n",
      "  5.1640957e-03  9.9245813e-03 -8.4572323e-03 -5.1396149e-03\n",
      " -7.0643406e-03 -4.8645870e-03 -3.7746020e-03 -8.5404655e-03\n",
      "  7.9557188e-03 -4.8424765e-03  8.4224204e-03  5.2635181e-03\n",
      " -6.5477407e-03  3.9594900e-03  5.4704030e-03 -7.4276445e-03\n",
      " -7.4025895e-03 -2.4750258e-03 -8.6241532e-03 -1.5805010e-03\n",
      " -3.9991256e-04  3.3005355e-03  1.4416134e-03 -8.8082597e-04\n",
      " -5.5941702e-03  1.7291227e-03 -9.0070191e-04  6.7971563e-03\n",
      "  3.9710999e-03  4.5290212e-03  1.4330813e-03 -2.7002366e-03\n",
      " -4.3681902e-03 -1.0307254e-03  1.4357518e-03 -2.6482302e-03\n",
      " -7.0773177e-03 -7.8051188e-03 -9.1207325e-03 -5.9341523e-03\n",
      " -1.8470909e-03 -4.3237079e-03 -6.4580250e-03 -3.7138180e-03\n",
      "  4.2845411e-03 -3.7393053e-03  8.3798347e-03  1.5346275e-03\n",
      " -7.2461781e-03  9.4384188e-03  7.6301363e-03  5.4912511e-03\n",
      " -6.8474049e-03  5.8231740e-03  4.0110410e-03  5.1870570e-03\n",
      "  4.2569772e-03  1.9438856e-03 -3.1686858e-03  8.3557209e-03\n",
      "  9.6144704e-03  3.7899534e-03 -2.8388954e-03  7.5329535e-06\n",
      "  1.2181019e-03 -8.4546134e-03 -8.2209306e-03 -2.2861801e-04\n",
      "  1.2350532e-03 -5.7422495e-03 -4.7259410e-03 -7.3466175e-03\n",
      "  8.3308648e-03  1.2123202e-04 -4.5054723e-03  5.7021524e-03\n",
      "  9.1815796e-03 -4.0980768e-03  7.9643140e-03  5.3767432e-03\n",
      "  5.8833393e-03  5.0972798e-04  8.2133794e-03 -7.0205205e-03]\n",
      "\n",
      "Word2Vec Skip-gram example for 'river':\n",
      " [ 1.0788626e-04  3.0728590e-03 -6.8074102e-03 -1.3744055e-03\n",
      "  7.6676249e-03  7.3310253e-03 -3.6665557e-03  2.6618114e-03\n",
      " -8.3244052e-03  6.1883265e-03 -4.6274741e-03 -3.1761671e-03\n",
      "  9.3089938e-03  8.7858428e-04  7.4931961e-03 -6.0655144e-03\n",
      "  5.1710894e-03  9.9285012e-03 -8.4521463e-03 -5.1429374e-03\n",
      " -7.0527913e-03 -4.8658769e-03 -3.7693323e-03 -8.5457051e-03\n",
      "  7.9575013e-03 -4.8414967e-03  8.4140329e-03  5.2696136e-03\n",
      " -6.5401220e-03  3.9561251e-03  5.4688719e-03 -7.4301050e-03\n",
      " -7.3963129e-03 -2.4790405e-03 -8.6234063e-03 -1.5866796e-03\n",
      " -3.8847685e-04  3.3037676e-03  1.4461229e-03 -8.8407565e-04\n",
      " -5.5963648e-03  1.7264698e-03 -9.0090901e-04  6.8033002e-03\n",
      "  3.9687487e-03  4.5332261e-03  1.4225909e-03 -2.6955886e-03\n",
      " -4.3605031e-03 -1.0262658e-03  1.4279818e-03 -2.6504160e-03\n",
      " -7.0803729e-03 -7.8071305e-03 -9.1127921e-03 -5.9303273e-03\n",
      " -1.8449230e-03 -4.3227677e-03 -6.4498503e-03 -3.7047297e-03\n",
      "  4.2763748e-03 -3.7353323e-03  8.3837481e-03  1.5384998e-03\n",
      " -7.2500398e-03  9.4510941e-03  7.6323934e-03  5.4972782e-03\n",
      " -6.8467110e-03  5.8235712e-03  4.0146387e-03  5.1869941e-03\n",
      "  4.2623701e-03  1.9418340e-03 -3.1606886e-03  8.3530769e-03\n",
      "  9.6194306e-03  3.7906950e-03 -2.8445793e-03  1.0971980e-06\n",
      "  1.2092076e-03 -8.4512476e-03 -8.2141608e-03 -2.2161345e-04\n",
      "  1.2372951e-03 -5.7399520e-03 -4.7187950e-03 -7.3472741e-03\n",
      "  8.3289966e-03  1.1664656e-04 -4.4961628e-03  5.6996462e-03\n",
      "  9.1870492e-03 -4.0883101e-03  7.9709450e-03  5.3787427e-03\n",
      "  5.8841100e-03  5.0759723e-04  8.2121659e-03 -7.0130122e-03]\n",
      "\n",
      "Average Word Embeddings for the example text (using CBOW):\n",
      " [-6.43752499e-04  5.36271515e-04  5.32585019e-05  1.04059337e-03\n",
      " -4.10694622e-04 -1.47853387e-03  1.11679709e-03  2.42849884e-03\n",
      " -1.08093241e-03 -1.70933674e-03  9.09310906e-04 -1.29634247e-03\n",
      " -9.48676697e-05  1.62807693e-03 -1.41981932e-04 -7.37265216e-04\n",
      "  2.13633911e-03  4.66594944e-04 -8.97693787e-04 -2.57344533e-03\n",
      "  8.69345175e-04  2.78753830e-04  2.15290562e-03 -8.82926528e-04\n",
      "  8.95174792e-04 -1.98911573e-04 -8.33660242e-04  1.19481470e-03\n",
      " -1.57364482e-03  3.48382396e-04  1.20183614e-03 -9.38743715e-04\n",
      "  1.10092813e-03 -3.09631928e-03 -5.11141222e-04  8.11125118e-04\n",
      "  1.17618750e-03 -4.40254011e-04 -5.52966972e-04 -4.28871565e-04\n",
      " -5.19577050e-04 -4.29707211e-04 -1.38959963e-03 -3.24257078e-04\n",
      "  7.98009676e-04 -5.59052747e-04 -8.32884714e-04 -1.59434010e-04\n",
      "  1.12092003e-03  1.15319675e-03  5.13120749e-05 -5.78059621e-04\n",
      " -8.80137290e-04 -4.82273929e-05  1.34477128e-04 -7.34853956e-05\n",
      "  1.28860235e-03 -6.62000016e-04 -1.01398262e-03  7.01636204e-04\n",
      " -3.42810966e-04 -8.53068673e-04  1.12133365e-03  1.37681109e-04\n",
      " -1.02919765e-03  2.21219359e-03 -5.07519797e-04  1.05840259e-03\n",
      " -1.55600389e-03  1.17118084e-03  1.26061299e-03  1.34078476e-03\n",
      "  1.51544745e-03  2.65633502e-04  1.44438542e-03 -3.09688508e-04\n",
      "  8.53094513e-04  6.33239381e-04 -4.05396919e-04 -9.00296610e-04\n",
      " -2.03536119e-03 -3.41118248e-04 -2.23165190e-04  1.08451067e-03\n",
      " -4.93398853e-04 -9.27770507e-04  1.61155196e-03 -4.72551766e-04\n",
      "  5.43704686e-04  9.17867742e-04  1.04790302e-03  2.89882659e-04\n",
      "  7.24103309e-04 -7.88157037e-04  1.93154271e-03  8.99262608e-04\n",
      " -1.64861794e-04 -1.04545134e-03 -3.93616987e-04  2.34292500e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Example text\n",
    "text = [\"The Kennebec River now runs mostly clean, thanks to laws that reduced pollution. \"\n",
    "        \"Yet four hydroelectric dams, two built in the early 20th century and the other two in the 1980s, \"\n",
    "        \"remain on the lower reaches of the 150-mile-long river and continue to prevent endangered salmon \"\n",
    "        \"from reaching their single most important spawning tributary, the Sandy River.\"]\n",
    "\n",
    "# Tokenizing the text\n",
    "tokens = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "one_hot = vectorizer.fit_transform(text).toarray()\n",
    "print(\"One-Hot Encoding:\\n\", one_hot)\n",
    "\n",
    "# Display One-Hot Encoding interim steps\n",
    "one_hot_vocab = vectorizer.vocabulary_\n",
    "print(\"\\nOne-Hot Encoding Vocabulary:\\n\", one_hot_vocab)\n",
    "\n",
    "# 2. Bag-of-Words\n",
    "bow_vectorizer = CountVectorizer()\n",
    "bow = bow_vectorizer.fit_transform(text).toarray()\n",
    "print(\"\\nBag-of-Words:\\n\", bow)\n",
    "\n",
    "# Display Bag-of-Words interim steps\n",
    "bow_vocab = bow_vectorizer.vocabulary_\n",
    "print(\"\\nBag-of-Words Vocabulary:\\n\", bow_vocab)\n",
    "\n",
    "# 3. TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(text).toarray()\n",
    "print(\"\\nTF-IDF:\\n\", tfidf)\n",
    "\n",
    "# Display TF-IDF interim steps\n",
    "tfidf_vocab = tfidf_vectorizer.vocabulary_\n",
    "print(\"\\nTF-IDF Vocabulary:\\n\", tfidf_vocab)\n",
    "\n",
    "# Calculate TF and IDF manually for illustration\n",
    "def compute_tf(text):\n",
    "    tf_text = []\n",
    "    for sentence in text:\n",
    "        tokens = word_tokenize(sentence.lower())\n",
    "        counter = Counter(tokens)\n",
    "        total_words = len(tokens)\n",
    "        tf_text.append({word: count / total_words for word, count in counter.items()})\n",
    "    return tf_text\n",
    "\n",
    "def compute_idf(text):\n",
    "    tokenized_text = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "    idf_dict = {}\n",
    "    N = len(tokenized_text)\n",
    "    all_tokens_set = set([item for sublist in tokenized_text for item in sublist])\n",
    "    \n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = sum([1 for sublist in tokenized_text if tkn in sublist])\n",
    "        idf_dict[tkn] = math.log(N / (1 + contains_token))\n",
    "    return idf_dict\n",
    "\n",
    "tf = compute_tf(text)\n",
    "idf = compute_idf(text)\n",
    "print(\"\\nTF:\\n\", tf)\n",
    "print(\"\\nIDF:\\n\", idf)\n",
    "\n",
    "# Tokenizing the text for Word2Vec\n",
    "tokens = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "\n",
    "# 4. Word2Vec CBOW (Continuous Bag of Words)\n",
    "cbow_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, sg=0)\n",
    "print(\"\\nWord2Vec CBOW example for 'river':\\n\", cbow_model.wv['river'])\n",
    "\n",
    "# 5. Word2Vec Skip-gram\n",
    "skipgram_model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, sg=1)\n",
    "print(\"\\nWord2Vec Skip-gram example for 'river':\\n\", skipgram_model.wv['river'])\n",
    "\n",
    "# 6. Average Word Embeddings (using Word2Vec CBOW)\n",
    "def average_word_vectors(tokens, model):\n",
    "    vector_size = model.vector_size\n",
    "    avg_vector = np.zeros((vector_size,))\n",
    "    num_words = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            avg_vector = np.add(avg_vector, model.wv[word])\n",
    "            num_words += 1\n",
    "    if num_words > 0:\n",
    "        avg_vector = np.divide(avg_vector, num_words)\n",
    "    return avg_vector\n",
    "\n",
    "avg_vector = average_word_vectors(tokens[0], cbow_model)\n",
    "print(\"\\nAverage Word Embeddings for the example text (using CBOW):\\n\", avg_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db09dc27-6108-41ad-83aa-623a810b42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2024.2.2)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/126.0 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/126.0 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 92.2/126.0 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 126.0/126.0 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/43.8 kB 640.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.8/43.8 kB 714.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.8/9.1 MB 38.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.5/9.1 MB 48.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.8/9.1 MB 55.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 58.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 401.7/401.7 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 287.3/287.3 kB ? eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.2/2.2 MB 68.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 68.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 20.1 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.2 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.1\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   - ------------------------------------- 20.5/626.3 kB 640.0 kB/s eta 0:00:01\n",
      "   ------------------ --------------------- 286.7/626.3 kB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 626.3/626.3 kB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "# These examples illustrate different methods for sentiment analysis:\n",
    "# Dictionary-Based Approach: Uses predefined sentiment lexicons to calculate sentiment scores.\n",
    "# Classifier-Based Approach: Uses machine learning models (e.g., BERT) to classify the sentiment.\n",
    "# Aspect Sentiment Modeling: Analyzes sentiments related to specific aspects or topics within the text.\n",
    "\n",
    "#!pip install vaderSentiment\n",
    "#!pip install transformers\n",
    "#!pip install textblob\n",
    "\n",
    "text = (\"The Kennebec River now runs mostly clean, thanks to laws that reduced pollution. \"\n",
    "        \"Yet four hydroelectric dams, two built in the early 20th century and the other two in the 1980s, \"\n",
    "        \"remain on the lower reaches of the 150-mile-long river and continue to prevent endangered salmon \"\n",
    "        \"from reaching their single most important spawning tributary, the Sandy River.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb37ca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary-Based Sentiment Analysis: {'neg': 0.034, 'neu': 0.781, 'pos': 0.184, 'compound': 0.7645}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary-based sentiment analysis using VADER\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "text = (\"The Kennebec River now runs mostly clean, thanks to laws that reduced pollution. \"\n",
    "        \"Yet four hydroelectric dams, two built in the early 20th century and the other two in the 1980s, \"\n",
    "        \"remain on the lower reaches of the 150-mile-long river and continue to prevent endangered salmon \"\n",
    "        \"from reaching their single most important spawning tributary, the Sandy River.\")\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentiment = analyzer.polarity_scores(text)\n",
    "\n",
    "print(f\"Dictionary-Based Sentiment Analysis: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78ff014",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a203a5bcd6401cb270c8ce22644fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Classifier-based sentiment analysis using a pre-trained model (e.g., BERT)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 4\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m sentiment \u001b[38;5;241m=\u001b[39m classifier(text)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifier-Based Sentiment Analysis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    905\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 906\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    907\u001b[0m         model,\n\u001b[0;32m    908\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    909\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    910\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    911\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    912\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    914\u001b[0m     )\n\u001b[0;32m    916\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    917\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:234\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    240\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "# Classifier-based sentiment analysis using a pre-trained model (e.g., BERT)\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('sentiment-analysis')\n",
    "sentiment = classifier(text)\n",
    "\n",
    "print(f\"Classifier-Based Sentiment Analysis: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2dc97b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect-Based Sentiment Analysis:\n",
      "Environment: Sentiment(polarity=0.2833333333333333, subjectivity=0.45000000000000007)\n",
      "Dams: Sentiment(polarity=0.13392857142857142, subjectivity=0.3982142857142857)\n"
     ]
    }
   ],
   "source": [
    "# Aspect-based sentiment analysis using a rule-based approach\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Splitting the text into different aspects\n",
    "aspects = {\n",
    "    \"environment\": \"The Kennebec River now runs mostly clean, thanks to laws that reduced pollution.\",\n",
    "    \"dams\": \"Yet four hydroelectric dams, two built in the early 20th century and the other two in the 1980s, \"\n",
    "            \"remain on the lower reaches of the 150-mile-long river and continue to prevent endangered salmon \"\n",
    "            \"from reaching their single most important spawning tributary, the Sandy River.\"\n",
    "}\n",
    "\n",
    "aspect_sentiments = {}\n",
    "for aspect, aspect_text in aspects.items():\n",
    "    blob = TextBlob(aspect_text)\n",
    "    sentiment = blob.sentiment\n",
    "    aspect_sentiments[aspect] = sentiment\n",
    "\n",
    "print(\"Aspect-Based Sentiment Analysis:\")\n",
    "for aspect, sentiment in aspect_sentiments.items():\n",
    "    print(f\"{aspect.capitalize()}: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e86d16",
   "metadata": {},
   "source": [
    "TextBlob computes subjectivity based on the presence of subjective terms and phrases in the text. The subjectivity score is a float within the range [0.0, 1.0], where 0.0 is very objective and 1.0 is very subjective. TextBlob uses a predefined lexicon of subjective terms to determine the subjectivity of a piece of text.\n",
    "\n",
    "Here’s a brief explanation of how TextBlob computes subjectivity:\n",
    "\n",
    "Lexicon-Based Analysis: TextBlob uses a lexicon of words that have been pre-labeled with subjectivity scores. These scores are derived from linguistic research and represent the degree to which a word expresses subjectivity.\n",
    "\n",
    "Term Frequency: The frequency of subjective terms in the text contributes to the overall subjectivity score. More frequent subjective terms lead to higher subjectivity scores.\n",
    "\n",
    "Phrase-Level Analysis: TextBlob also considers phrases and not just individual words. Some phrases might convey subjectivity differently from their constituent words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
