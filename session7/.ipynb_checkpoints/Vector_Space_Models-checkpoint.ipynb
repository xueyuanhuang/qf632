{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e6993de",
   "metadata": {},
   "source": [
    "We are using  CountVectorizer for sparse vector representations and the Word2Vec model from the gensim library for dense vector representations.\n",
    "\n",
    "Sparse Vector Space Model\n",
    "A sparse vector space model represents text as high-dimensional vectors, where each dimension corresponds to a specific word in the vocabulary. We'll use CountVectorizer from sklearn to create a sparse representation.\n",
    "\n",
    "Dense Vector Space Model\n",
    "A dense vector space model represents text in a lower-dimensional continuous vector space, where similar words have similar vector representations. We'll use Word2Vec from gensim to create a dense representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c479cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Representation (Sparse Vector Space Model):\n",
      "   artificial  as  be  but  field  fun  intelligence  is  it  language  ...  \\\n",
      "0           0   0   1    0      0    0             0   0   1         0  ...   \n",
      "1           0   2   1    1      0    2             0   1   1         1  ...   \n",
      "2           1   0   0    0      1    0             1   1   0         0  ...   \n",
      "\n",
      "   love  machine  natural  not  of  processing  should  taught  think  widely  \n",
      "0     1        1        0    0   0           0       1       1      0       1  \n",
      "1     0        0        1    1   0           1       1       0      1       0  \n",
      "2     0        1        0    0   1           0       0       0      0       0  \n",
      "\n",
      "[3 rows x 21 columns]\n",
      "\n",
      "One-Hot Encoding Representation:\n",
      "   artificial   as   be  but  field  fun  intelligence   is   it  language  \\\n",
      "0         0.0  0.0  1.0  0.0    0.0  0.0           0.0  0.0  1.0       0.0   \n",
      "1         0.0  1.0  1.0  1.0    0.0  1.0           0.0  1.0  1.0       1.0   \n",
      "2         1.0  0.0  0.0  0.0    1.0  0.0           1.0  1.0  0.0       0.0   \n",
      "\n",
      "   ...  love  machine  natural  not   of  processing  should  taught  think  \\\n",
      "0  ...   1.0      1.0      0.0  0.0  0.0         0.0     1.0     1.0    0.0   \n",
      "1  ...   0.0      0.0      0.0  1.0  0.0         1.0     1.0     0.0    1.0   \n",
      "2  ...   0.0      0.0      0.0  0.0  1.0         0.0     0.0     0.0    0.0   \n",
      "\n",
      "   widely  \n",
      "0     1.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "\n",
      "[3 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love machine learning, it should be taught widely\",\n",
    "    \"Natural language processing is fun but not as fun as I think it should be\",\n",
    "    \"Machine learning is a field of artificial intelligence\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_sparse = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_sparse = pd.DataFrame(X_sparse.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Representation (Sparse Vector Space Model):\")\n",
    "print(df_sparse)\n",
    "\n",
    "# One-Hot Encoding\n",
    "def one_hot_encode(corpus, vocab):\n",
    "    one_hot_vectors = []\n",
    "    vocab_list = list(vocab)\n",
    "    for doc in corpus:\n",
    "        vector = np.zeros(len(vocab_list))\n",
    "        for word in doc.split():\n",
    "            if word in vocab_list:\n",
    "                vector[vocab_list.index(word)] = 1\n",
    "        one_hot_vectors.append(vector)\n",
    "    return np.array(one_hot_vectors)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# One-hot encode the corpus\n",
    "one_hot_vectors = one_hot_encode(corpus, vocab)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_one_hot = pd.DataFrame(one_hot_vectors, columns=vocab)\n",
    "print(\"\\nOne-Hot Encoding Representation:\")\n",
    "print(df_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8bd1592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Corpus:\n",
      "[['love', 'machine', 'learning'], ['natural', 'language', 'processing', 'is', 'fun'], ['machine', 'learning', 'is', 'field', 'of', 'artificial', 'intelligence']]\n",
      "\n",
      "Word2Vec Model Trained.\n",
      "\n",
      "Dense Vector for the word 'machine':\n",
      "[ 0.07311766  0.05070262  0.06757693  0.00762866  0.06350891 -0.03405366\n",
      " -0.00946401  0.05768573 -0.07521638 -0.03936104]\n",
      "\n",
      "Dense Vectors for the entire vocabulary:\n",
      "is: [-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n",
      "  0.06458873  0.08972988 -0.05015428 -0.03763372]\n",
      "learning: [ 0.07381326 -0.01533646 -0.04537105  0.0655477  -0.04860705 -0.01816211\n",
      "  0.02876889  0.00991992 -0.08286119 -0.09449868]\n",
      "machine: [ 0.07311766  0.05070262  0.06757693  0.00762866  0.06350891 -0.03405366\n",
      " -0.00946401  0.05768573 -0.07521638 -0.03936104]\n",
      "intelligence: [-0.07511582 -0.00930042  0.09538119 -0.07319167 -0.02333769 -0.01937741\n",
      "  0.08077437 -0.05930896  0.00045162 -0.04753734]\n",
      "artificial: [-0.0960355   0.05007293 -0.08759586 -0.04391825 -0.000351   -0.00296181\n",
      " -0.0766124   0.09614743  0.04982058  0.09233143]\n",
      "of: [-0.08158365  0.04496045 -0.04137303  0.00824581  0.08499086 -0.04462421\n",
      "  0.04517748 -0.06787333 -0.03548684  0.09399024]\n",
      "field: [-0.01577653  0.00321372 -0.0414063  -0.07682689 -0.01508008  0.02469795\n",
      " -0.00888027  0.05533662 -0.02742977  0.02260065]\n",
      "fun: [ 0.05455794  0.08345953 -0.01453741 -0.09208143  0.04370552  0.00571785\n",
      "  0.07441908 -0.00813283 -0.02638414 -0.08753009]\n",
      "processing: [-0.00856557  0.02826563  0.05401429  0.07052656 -0.05703121  0.0185882\n",
      "  0.06088864 -0.04798051 -0.03107261  0.0679763 ]\n",
      "language: [ 0.01631476  0.00189917  0.03473637  0.00217777  0.09618826  0.05060603\n",
      " -0.0891739  -0.0704156   0.00901456  0.06392534]\n",
      "natural: [-0.08619688  0.03665738  0.05189884  0.05741938  0.07466918 -0.06167675\n",
      "  0.01105614  0.06047282 -0.0284005  -0.06173522]\n",
      "love: [-0.00410223 -0.08368949 -0.05600012  0.07104538  0.0335254   0.0722567\n",
      "  0.06800248  0.07530741 -0.03789154 -0.00561806]\n",
      "\n",
      "Cosine Similarity between 'machine' and 'learning': 0.3293722867965698\n",
      "\n",
      "Cosine Similarity Matrix for the entire vocabulary:\n",
      "                    is  learning   machine  intelligence  artificial  \\\n",
      "is            1.000000  0.543601  0.300425      0.104944   -0.224187   \n",
      "learning      0.543601  1.000000  0.329372     -0.211337   -0.538184   \n",
      "machine       0.300425  0.329372  1.000000     -0.105510   -0.320797   \n",
      "intelligence  0.104944 -0.211337 -0.105510      1.000000   -0.366271   \n",
      "artificial   -0.224187 -0.538184 -0.320797     -0.366271    1.000000   \n",
      "of           -0.272602 -0.382054 -0.151690      0.092673    0.249538   \n",
      "field        -0.189738 -0.179987 -0.089375     -0.113875    0.614398   \n",
      "fun          -0.131116  0.232430  0.427315      0.294122   -0.287867   \n",
      "processing    0.379290  0.035253 -0.112888      0.207132   -0.301764   \n",
      "language     -0.728746 -0.514574  0.055418     -0.231572    0.042648   \n",
      "natural       0.431825  0.033765  0.470530      0.199036    0.026828   \n",
      "love          0.227431  0.358688 -0.043767     -0.310591   -0.065713   \n",
      "\n",
      "                    of     field       fun  processing  language   natural  \\\n",
      "is           -0.272602 -0.189738 -0.131116    0.379290 -0.728746  0.431825   \n",
      "learning     -0.382054 -0.179987  0.232430    0.035253 -0.514574  0.033765   \n",
      "machine      -0.151690 -0.089375  0.427315   -0.112888  0.055418  0.470530   \n",
      "intelligence  0.092673 -0.113875  0.294122    0.207132 -0.231572  0.199036   \n",
      "artificial    0.249538  0.614398 -0.287867   -0.301764  0.042648  0.026828   \n",
      "of            1.000000 -0.042645 -0.021817    0.275562  0.291413  0.223849   \n",
      "field        -0.042645  1.000000  0.197344   -0.390177 -0.198632 -0.246846   \n",
      "fun          -0.021817  0.197344  1.000000   -0.277511 -0.217994  0.051000   \n",
      "processing    0.275562 -0.390177 -0.277511    1.000000 -0.021763 -0.083090   \n",
      "language      0.291413 -0.198632 -0.217994   -0.021763  1.000000 -0.152503   \n",
      "natural       0.223849 -0.246846  0.051000   -0.083090 -0.152503  1.000000   \n",
      "love         -0.060955  0.115342 -0.150896    0.014488 -0.226438  0.098239   \n",
      "\n",
      "                  love  \n",
      "is            0.227431  \n",
      "learning      0.358688  \n",
      "machine      -0.043767  \n",
      "intelligence -0.310591  \n",
      "artificial   -0.065713  \n",
      "of           -0.060955  \n",
      "field         0.115342  \n",
      "fun          -0.150896  \n",
      "processing    0.014488  \n",
      "language     -0.226438  \n",
      "natural       0.098239  \n",
      "love          1.000000  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love machine learning\",\n",
    "    \"Natural language processing is fun\",\n",
    "    \"Machine learning is a field of artificial intelligence\"\n",
    "]\n",
    "\n",
    "# Step 1: Preprocess the corpus\n",
    "corpus_processed = [simple_preprocess(doc) for doc in corpus]\n",
    "print(\"Preprocessed Corpus:\")\n",
    "print(corpus_processed)\n",
    "\n",
    "# Step 2: Initialize and train Word2Vec model\n",
    "model = Word2Vec(sentences=corpus_processed, vector_size=10, window=5, min_count=1, workers=4)\n",
    "print(\"\\nWord2Vec Model Trained.\")\n",
    "\n",
    "# Step 3: Retrieve the dense vector for a specific word\n",
    "word = 'machine'\n",
    "vector_dense = model.wv[word]\n",
    "print(f\"\\nDense Vector for the word '{word}':\")\n",
    "print(vector_dense)\n",
    "\n",
    "# Step 4: Retrieve vectors for all words in the vocabulary\n",
    "vocabulary = model.wv.key_to_index\n",
    "vectors_dense = {word: model.wv[word] for word in vocabulary}\n",
    "print(\"\\nDense Vectors for the entire vocabulary:\")\n",
    "for word, vector in vectors_dense.items():\n",
    "    print(f\"{word}: {vector}\")\n",
    "\n",
    "# Step 5: Compute cosine similarity between two words\n",
    "word1 = 'machine'\n",
    "word2 = 'learning'\n",
    "vector1 = model.wv[word1].reshape(1, -1)\n",
    "vector2 = model.wv[word2].reshape(1, -1)\n",
    "\n",
    "cos_sim = cosine_similarity(vector1, vector2)\n",
    "print(f\"\\nCosine Similarity between '{word1}' and '{word2}': {cos_sim[0][0]}\")\n",
    "\n",
    "# Step 6: Compute cosine similarity matrix for the entire vocabulary\n",
    "vocab_vectors = np.array([model.wv[word] for word in vocabulary])\n",
    "cos_sim_matrix = cosine_similarity(vocab_vectors)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "df_cos_sim = pd.DataFrame(cos_sim_matrix, index=vocabulary, columns=vocabulary)\n",
    "print(\"\\nCosine Similarity Matrix for the entire vocabulary:\")\n",
    "print(df_cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905bac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
